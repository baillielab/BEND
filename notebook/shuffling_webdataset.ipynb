{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "318dc53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import webdataset as wds\n",
    "from tqdm import tqdm\n",
    "from bend.utils.data_downstream import collate_fn_pad_to_longest\n",
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "data_dir = '../data/cpg_methylation/hyenadna-tiny-1k'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd62f9b",
   "metadata": {},
   "source": [
    "### Creating shards\n",
    "\n",
    "The `chunk_size` parameter in `embed.yml` defines the shard size.\n",
    "\n",
    "Annotation data is split into chunks of the given `chunk size`, each chunk is embedded by processing samples sequentially and saved into a `shard` (a tar file).\n",
    "\n",
    "Default `chunk_size` is 50,000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f88bd",
   "metadata": {},
   "source": [
    "#### shuffling shards\n",
    "\n",
    "Shards are nto shuffled explicitly (or maybe knowingly): while shards are names after `split_chunk-id`, they are read by `glob.glob(path_to_shards)`. Hence, they are NOT in ascending order by `chunk_id`, which is a degree of 'shuffling'.\n",
    "\n",
    "Following this behaviour, the more the shards, the more the shuffling.\n",
    "\n",
    "The proper way of shuffling at shard level is to set `shardShuffle=True` when creating the `WebDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "716cff4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/cpg_methylation/hyenadna-tiny-1k/test_1.tar.gz',\n",
       " '../data/cpg_methylation/hyenadna-tiny-1k/test_2.tar.gz',\n",
       " '../data/cpg_methylation/hyenadna-tiny-1k/test_0.tar.gz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 'test'\n",
    "\n",
    "tars = glob.glob(f\"{data_dir}/*.tar.gz\")\n",
    "data = [x for x in tars if os.path.split(x)[-1].startswith(split)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27100d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_50000\n",
      "sample_50001\n",
      "sample_50002\n",
      "sample_50003\n",
      "sample_50004\n",
      "sample_50005\n",
      "sample_50006\n",
      "sample_50007\n",
      "sample_50008\n",
      "sample_50009\n",
      "sample_50010\n"
     ]
    }
   ],
   "source": [
    "# shardShuffle is set to None in the original code, which would lead to not be shuffled.\n",
    "# However, this raises a warning asking to be set explicitly to False, True or a number.\n",
    "# To avoid the warning and keep the original behavior, we set it to False.\n",
    "\n",
    "dataset = wds.WebDataset(data, shardshuffle=False)\n",
    "dataset = dataset.decode()  \n",
    "\n",
    "dataloader = wds.WebLoader(dataset, num_workers=0, batch_size=None)\n",
    "\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    print(batch['__key__'])\n",
    "    \n",
    "    if idx == 10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28287a6a",
   "metadata": {},
   "source": [
    "### In-memory shuffling\n",
    "Each worker sequentially loads samples from one or more shards into batches (cannot load from 0 shards, will throw an error - hence make sure num workers<= num of shards).\n",
    "\n",
    "By using `.shuffle(buffer_size)`, a buffer can be created so that samples are loaded into it first, then randomly distributed into batches.\n",
    "\n",
    "Having a buffer size = to the number of samples into a shard allow to completely shuffle such shard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9bfecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_50062\n",
      "sample_50088\n",
      "sample_50079\n",
      "sample_50025\n",
      "sample_50070\n",
      "sample_50080\n",
      "sample_50007\n",
      "sample_50067\n",
      "sample_50200\n",
      "sample_50179\n",
      "sample_50156\n"
     ]
    }
   ],
   "source": [
    "dataset = wds.WebDataset(data, shardshuffle=False)\n",
    "\n",
    "\n",
    "# Explanation of the initial argument: https://github.com/webdataset/webdataset/issues/62\n",
    "# Basically allows to await streaming data until the given amount of samples are shuffled.\n",
    "buffer_size = 200\n",
    "dataset = dataset.shuffle(buffer_size, initial=buffer_size)\n",
    "\n",
    "dataset = dataset.decode()\n",
    "\n",
    "dataloader = wds.WebLoader(dataset, num_workers=0, batch_size=None)\n",
    "\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    print(batch['__key__'])\n",
    "    \n",
    "    if idx == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca609c",
   "metadata": {},
   "source": [
    "### Multiprocessing shuffling\n",
    "\n",
    "When the dataloader's number of workers parameter is set to 0, one shard is loaded into memory at a time. Consequently, the shards are accessed in the order in which the `.tar` files where loaded by `glob.glob()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9db507cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/cpg_methylation/hyenadna-tiny-1k/test_1.tar.gz',\n",
       " '../data/cpg_methylation/hyenadna-tiny-1k/test_2.tar.gz',\n",
       " '../data/cpg_methylation/hyenadna-tiny-1k/test_0.tar.gz']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33600db",
   "metadata": {},
   "source": [
    "However, increasing the number of workers to values greaten than 0, enables multiprocessing, and the number of shards accessed simultaneously is equal to the number of workers.\n",
    "\n",
    "This greatly increases randomisation, as samples of multiple shards are processed at once. If the number of shards = the number of workers, it allows to use samples from any __section__ of the dataset. \n",
    "\n",
    "However, each shard/section is accessed sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b82d759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_50000\n",
      "sample_100000\n",
      "sample_0\n",
      "sample_50001\n",
      "sample_100001\n",
      "sample_1\n",
      "sample_50002\n",
      "sample_100002\n",
      "sample_2\n",
      "sample_50003\n",
      "sample_100003\n"
     ]
    }
   ],
   "source": [
    "dataset = wds.WebDataset(data, shardshuffle=False)\n",
    "dataset = dataset.decode()\n",
    "\n",
    "\n",
    "dataloader = wds.WebLoader(dataset, num_workers=len(data), batch_size=None)\n",
    "\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    print(batch['__key__'])\n",
    "    \n",
    "    if idx == 10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbadbc6e",
   "metadata": {},
   "source": [
    "Of course the `dataset.shuffle(buffer_size)` function can be used to allow the shuffling of the first N samples of each shard, where N=buffer_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48201efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_50079\n",
      "sample_100144\n",
      "sample_83\n",
      "sample_50058\n",
      "sample_100064\n",
      "sample_41\n",
      "sample_50190\n",
      "sample_100020\n",
      "sample_198\n",
      "sample_50093\n",
      "sample_100072\n"
     ]
    }
   ],
   "source": [
    "dataset = wds.WebDataset(data, shardshuffle=False)\n",
    "\n",
    "buffer_size = 200\n",
    "dataset = dataset.shuffle(buffer_size, initial=buffer_size)\n",
    "dataset = dataset.decode()\n",
    "\n",
    "dataloader = wds.WebLoader(dataset, num_workers=len(data), batch_size=None)\n",
    "\n",
    "for idx, batch in enumerate(dataloader):\n",
    "    print(batch['__key__'])\n",
    "    \n",
    "    if idx == 10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80d8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
