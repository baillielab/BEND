{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d59d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dgreco2/miniconda3/envs/bend/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/dgreco2/miniconda3/envs/bend/lib/python3.10/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from bend.models.hyena_dna import HyenaDNAPreTrainedModel, CharacterTokenizer\n",
    "import os\n",
    "from utils import generate_random_dna_sequence, get_device, chunkify_sequences, process_chunk_embeddings\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "WORK_PATH = '../../'\n",
    "EMBEDDER_DIR = os.path.join(WORK_PATH, 'pretrained_models', 'hyenadna')\n",
    "EMBEDDER_NAME = 'hyenadna-tiny-1k-seqlen'\n",
    "EMBEDDER_PATH = os.path.join(EMBEDDER_DIR, EMBEDDER_NAME)\n",
    "\n",
    "PADDING_VALUE = -100\n",
    "\n",
    "MAX_LENGTHS = {\n",
    "    \"hyenadna-tiny-1k-seqlen\": 1024,\n",
    "    \"hyenadna-small-32k-seqlen\": 32768,\n",
    "    \"hyenadna-medium-160k-seqlen\": 160000,\n",
    "    \"hyenadna-medium-450k-seqlen\": 450000,\n",
    "    \"hyenadna-large-1m-seqlen\": 1_000_000,\n",
    "}\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f38af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharacterTokenizer(\n",
    "    characters=[\"A\", \"C\", \"G\", \"T\", \"N\"],  # add DNA characters, N is uncertain\n",
    "    model_max_length=MAX_LENGTHS[EMBEDDER_NAME]\n",
    "    + 2,  # to account for special tokens, like EOS\n",
    "    add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "    padding_side=\"left\",  # since HyenaDNA is causal, we pad on the left\n",
    ")\n",
    "\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained(\n",
    "    os.path.join(EMBEDDER_DIR, EMBEDDER_NAME),\n",
    "    EMBEDDER_NAME,\n",
    "    download=not os.path.exists(EMBEDDER_DIR),\n",
    "    config=None,\n",
    "    device=device,\n",
    "    use_head=False,\n",
    "    use_lm_head=False,  # we don't use the LM head for embeddings\n",
    "    n_classes=2,\n",
    ").eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec6cfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CACCGAACAGCGGCG',\n",
       " 'CCTGTTAACGT',\n",
       " 'TATGTAGG',\n",
       " 'ACACCTCACCCCG',\n",
       " 'TCCATCAAAACGCAG',\n",
       " 'AGCACT',\n",
       " 'ACGGTCCAGAACCGC',\n",
       " 'CGTAA',\n",
       " 'AGCTTGGGGC',\n",
       " 'GTTAGCCAT']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [generate_random_dna_sequence(min_length=5, max_length=15) for _ in range(10)]\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d310e59a",
   "metadata": {},
   "source": [
    "#### Divide into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b76d762b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk ID: 0, Sequence: CACC\n",
      "Chunk ID: 0, Sequence: GAAC\n",
      "Chunk ID: 0, Sequence: AGCG\n",
      "Chunk ID: 0, Sequence: GCG\n",
      "Chunk ID: 1, Sequence: CCTG\n",
      "Chunk ID: 1, Sequence: TTAA\n",
      "Chunk ID: 1, Sequence: CGT\n",
      "Chunk ID: 2, Sequence: TATG\n",
      "Chunk ID: 2, Sequence: TAGG\n",
      "Chunk ID: 3, Sequence: ACAC\n",
      "Chunk ID: 3, Sequence: CTCA\n",
      "Chunk ID: 3, Sequence: CCCC\n",
      "Chunk ID: 3, Sequence: G\n",
      "Chunk ID: 4, Sequence: TCCA\n",
      "Chunk ID: 4, Sequence: TCAA\n",
      "Chunk ID: 4, Sequence: AACG\n",
      "Chunk ID: 4, Sequence: CAG\n",
      "Chunk ID: 5, Sequence: AGCA\n",
      "Chunk ID: 5, Sequence: CT\n",
      "Chunk ID: 6, Sequence: ACGG\n",
      "Chunk ID: 6, Sequence: TCCA\n",
      "Chunk ID: 6, Sequence: GAAC\n",
      "Chunk ID: 6, Sequence: CGC\n",
      "Chunk ID: 7, Sequence: CGTA\n",
      "Chunk ID: 7, Sequence: A\n",
      "Chunk ID: 8, Sequence: AGCT\n",
      "Chunk ID: 8, Sequence: TGGG\n",
      "Chunk ID: 8, Sequence: GC\n",
      "Chunk ID: 9, Sequence: GTTA\n",
      "Chunk ID: 9, Sequence: GCCA\n",
      "Chunk ID: 9, Sequence: T\n"
     ]
    }
   ],
   "source": [
    "MAX_MODEL_LENGTH = 4\n",
    "chunked_sequences, chunk_ids = chunkify_sequences(sequences, MAX_MODEL_LENGTH)\n",
    "\n",
    "for seq, chunk_id in zip(chunked_sequences, chunk_ids):\n",
    "    print(f\"Chunk ID: {chunk_id}, Sequence: {seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c52b25",
   "metadata": {},
   "source": [
    "#### Tokenise sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60673588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  8,  7,  8,  8,  1],\n",
       "        [ 0,  9,  7,  7,  8,  1],\n",
       "        [ 0,  7,  9,  8,  9,  1],\n",
       "        [ 4,  0,  9,  8,  9,  1],\n",
       "        [ 0,  8,  8, 10,  9,  1],\n",
       "        [ 0, 10, 10,  7,  7,  1],\n",
       "        [ 4,  0,  8,  9, 10,  1],\n",
       "        [ 0, 10,  7, 10,  9,  1],\n",
       "        [ 0, 10,  7,  9,  9,  1],\n",
       "        [ 0,  7,  8,  7,  8,  1],\n",
       "        [ 0,  8, 10,  8,  7,  1],\n",
       "        [ 0,  8,  8,  8,  8,  1],\n",
       "        [ 4,  4,  4,  0,  9,  1],\n",
       "        [ 0, 10,  8,  8,  7,  1],\n",
       "        [ 0, 10,  8,  7,  7,  1],\n",
       "        [ 0,  7,  7,  8,  9,  1],\n",
       "        [ 4,  0,  8,  7,  9,  1],\n",
       "        [ 0,  7,  9,  8,  7,  1],\n",
       "        [ 4,  4,  0,  8, 10,  1],\n",
       "        [ 0,  7,  8,  9,  9,  1],\n",
       "        [ 0, 10,  8,  8,  7,  1],\n",
       "        [ 0,  9,  7,  7,  8,  1],\n",
       "        [ 4,  0,  8,  9,  8,  1],\n",
       "        [ 0,  8,  9, 10,  7,  1],\n",
       "        [ 4,  4,  4,  0,  7,  1],\n",
       "        [ 0,  7,  9,  8, 10,  1],\n",
       "        [ 0, 10,  9,  9,  9,  1],\n",
       "        [ 4,  4,  0,  9,  8,  1],\n",
       "        [ 0,  9, 10, 10,  7,  1],\n",
       "        [ 0,  9,  8,  8,  7,  1],\n",
       "        [ 4,  4,  4,  0, 10,  1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer(\n",
    "    chunked_sequences,\n",
    "    return_tensors=\"pt\",\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"longest\",\n",
    ")\n",
    "\n",
    "input_ids = output[\"input_ids\"]\n",
    "attention_mask = output[\"attention_mask\"]\n",
    "\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19ec0961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'C', 'A', 'C', 'C', '[SEP]']\n",
      "['[CLS]', 'G', 'A', 'A', 'C', '[SEP]']\n",
      "['[CLS]', 'A', 'G', 'C', 'G', '[SEP]']\n",
      "['[PAD]', '[CLS]', 'G', 'C', 'G', '[SEP]']\n",
      "['[CLS]', 'C', 'C', 'T', 'G', '[SEP]']\n",
      "['[CLS]', 'T', 'T', 'A', 'A', '[SEP]']\n",
      "['[PAD]', '[CLS]', 'C', 'G', 'T', '[SEP]']\n",
      "['[CLS]', 'T', 'A', 'T', 'G', '[SEP]']\n",
      "['[CLS]', 'T', 'A', 'G', 'G', '[SEP]']\n",
      "['[CLS]', 'A', 'C', 'A', 'C', '[SEP]']\n",
      "['[CLS]', 'C', 'T', 'C', 'A', '[SEP]']\n",
      "['[CLS]', 'C', 'C', 'C', 'C', '[SEP]']\n",
      "['[PAD]', '[PAD]', '[PAD]', '[CLS]', 'G', '[SEP]']\n",
      "['[CLS]', 'T', 'C', 'C', 'A', '[SEP]']\n",
      "['[CLS]', 'T', 'C', 'A', 'A', '[SEP]']\n",
      "['[CLS]', 'A', 'A', 'C', 'G', '[SEP]']\n",
      "['[PAD]', '[CLS]', 'C', 'A', 'G', '[SEP]']\n",
      "['[CLS]', 'A', 'G', 'C', 'A', '[SEP]']\n",
      "['[PAD]', '[PAD]', '[CLS]', 'C', 'T', '[SEP]']\n",
      "['[CLS]', 'A', 'C', 'G', 'G', '[SEP]']\n",
      "['[CLS]', 'T', 'C', 'C', 'A', '[SEP]']\n",
      "['[CLS]', 'G', 'A', 'A', 'C', '[SEP]']\n",
      "['[PAD]', '[CLS]', 'C', 'G', 'C', '[SEP]']\n",
      "['[CLS]', 'C', 'G', 'T', 'A', '[SEP]']\n",
      "['[PAD]', '[PAD]', '[PAD]', '[CLS]', 'A', '[SEP]']\n",
      "['[CLS]', 'A', 'G', 'C', 'T', '[SEP]']\n",
      "['[CLS]', 'T', 'G', 'G', 'G', '[SEP]']\n",
      "['[PAD]', '[PAD]', '[CLS]', 'G', 'C', '[SEP]']\n",
      "['[CLS]', 'G', 'T', 'T', 'A', '[SEP]']\n",
      "['[CLS]', 'G', 'C', 'C', 'A', '[SEP]']\n",
      "['[PAD]', '[PAD]', '[PAD]', '[CLS]', 'T', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "for ids in input_ids:\n",
    "    print(tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93db1e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8277f",
   "metadata": {},
   "source": [
    "#### Embed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f95ec94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 6, 128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.LongTensor(input_ids)\n",
    "\n",
    "embeddings = model(input_ids=input_ids.to(device)).detach().cpu().numpy()\n",
    "input_ids = input_ids.numpy()\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "523f6980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16824079,  0.04744038,  0.04514444,  0.19083378,  0.05306307,\n",
       "       -0.19100481,  0.13585165,  0.49546304, -0.11520602,  0.35716903,\n",
       "       -0.50841016, -0.03071012, -0.33436307, -0.3122338 , -0.5464077 ,\n",
       "       -0.23307246, -0.33157188,  0.02428012,  0.2100084 , -0.37095368,\n",
       "       -0.3122338 ,  0.04744038, -0.04458451,  0.42001677, -0.2036671 ,\n",
       "        0.61932755, -0.2555642 , -0.07029043,  0.30389643, -0.2047453 ,\n",
       "       -0.21036223], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[:, -1, -1]  # last token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abfe137",
   "metadata": {},
   "source": [
    "#### Post-process chunks into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0edc954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk ID: 0\n",
      "Sequence: CACCGAACAGCGGCG  Length: 15\n",
      "  Tokens: CACCGAACAGCGGCG\n",
      "Embedding shape: (15, 128)\n",
      "Chunk ID: 1\n",
      "Sequence: CCTGTTAACGT  Length: 11\n",
      "  Tokens: CCTGTTAACGT\n",
      "Embedding shape: (11, 128)\n",
      "Chunk ID: 2\n",
      "Sequence: TATGTAGG  Length: 8\n",
      "  Tokens: TATGTAGG\n",
      "Embedding shape: (8, 128)\n",
      "Chunk ID: 3\n",
      "Sequence: ACACCTCACCCCG  Length: 13\n",
      "  Tokens: ACACCTCACCCCG\n",
      "Embedding shape: (13, 128)\n",
      "Chunk ID: 4\n",
      "Sequence: TCCATCAAAACGCAG  Length: 15\n",
      "  Tokens: TCCATCAAAACGCAG\n",
      "Embedding shape: (15, 128)\n",
      "Chunk ID: 5\n",
      "Sequence: AGCACT  Length: 6\n",
      "  Tokens: AGCACT\n",
      "Embedding shape: (6, 128)\n",
      "Chunk ID: 6\n",
      "Sequence: ACGGTCCAGAACCGC  Length: 15\n",
      "  Tokens: ACGGTCCAGAACCGC\n",
      "Embedding shape: (15, 128)\n",
      "Chunk ID: 7\n",
      "Sequence: CGTAA  Length: 5\n",
      "  Tokens: CGTAA\n",
      "Embedding shape: (5, 128)\n",
      "Chunk ID: 8\n",
      "Sequence: AGCTTGGGGC  Length: 10\n",
      "  Tokens: AGCTTGGGGC\n",
      "Embedding shape: (10, 128)\n",
      "Chunk ID: 9\n",
      "Sequence: GTTAGCCAT  Length: 9\n",
      "  Tokens: GTTAGCCAT\n",
      "Embedding shape: (9, 128)\n"
     ]
    }
   ],
   "source": [
    "sequence_embeddings, masked_tokens = process_chunk_embeddings(tokenizer, embeddings, input_ids, chunk_ids, upsample=False)\n",
    "\n",
    "for i, seq_emb in enumerate(sequence_embeddings):\n",
    "    assert sequences[i] == ''.join(masked_tokens[i]), f\"Mismatch in sequence {i}: {sequences[i]} != {''.join(masked_tokens[i])}\"\n",
    "    print(f\"Chunk ID: {np.unique(chunk_ids)[i]}\")\n",
    "    print(f\"Sequence: {sequences[i]}  Length: {len(sequences[i])}\")\n",
    "    print(f\"  Tokens: {''.join(masked_tokens[i])}\")\n",
    "    print(f\"Embedding shape: {seq_emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63878ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
