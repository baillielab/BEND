{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "318dc53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import webdataset as wds\n",
    "import os\n",
    "\n",
    "data_dir = '../data/cpg_methylation/hyenadna-tiny-1k'\n",
    "\n",
    "def print_samples_id(dataloader, n_samples=10):\n",
    "    for idx, sample in enumerate(dataloader):\n",
    "        print(sample['__key__'])\n",
    "        \n",
    "        if idx == n_samples-1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc44473",
   "metadata": {},
   "source": [
    "# Types of shuffling in WebDataset\n",
    "\n",
    "Usually, a first shuffle is given to the entirety of the dataset. This allows to break the positional relationships between samples.\n",
    "\n",
    "However, this step is not performed in the BEND benchmark. Instead, the sorted, by chromosome and sequence position, annotation .bed file is used for the embeddings. Hence sequences are embedded sequentially and saved into WebDataset shards while keeping their positional order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd62f9b",
   "metadata": {},
   "source": [
    "### Creating shards\n",
    "\n",
    "The `chunk_size` parameter in `embed.yml` defines the shard size.\n",
    "\n",
    "Annotation data is split into chunks of the given `chunk size`, each chunk is embedded by processing samples sequentially and saved into a `shard` (a tar file).\n",
    "\n",
    "Default `chunk_size` is 50,000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f88bd",
   "metadata": {},
   "source": [
    "#### shuffling shards\n",
    "\n",
    "Shards are nto shuffled explicitly (or maybe knowingly): while shards are names after `split_chunk-id`, they are read by `glob.glob(path_to_shards)`. Hence, they are NOT in ascending order by `chunk_id`, which is a degree of 'shuffling'.\n",
    "\n",
    "Following this behaviour, the more the shards, the more the shuffling.\n",
    "\n",
    "The proper way of shuffling at shard level is to set `shardShuffle= # shards` when creating the `WebDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "716cff4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/cpg_methylation/hyenadna-tiny-1k/test_1.tar.gz',\n",
       " '../data/cpg_methylation/hyenadna-tiny-1k/test_2.tar.gz',\n",
       " '../data/cpg_methylation/hyenadna-tiny-1k/test_0.tar.gz']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 'test'\n",
    "\n",
    "tars = glob.glob(f\"{data_dir}/*.tar.gz\")\n",
    "data = [x for x in tars if os.path.split(x)[-1].startswith(split)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27100d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_50000\n",
      "sample_50001\n",
      "sample_50002\n",
      "sample_50003\n",
      "sample_50004\n",
      "sample_50005\n",
      "sample_50006\n",
      "sample_50007\n",
      "sample_50008\n",
      "sample_50009\n"
     ]
    }
   ],
   "source": [
    "# shardShuffle is set to None in the original code, which would lead to not be shuffled.\n",
    "# However, this raises a warning asking to be set explicitly to False or a number.\n",
    "# To avoid the warning and keep the original behavior, we set it to False.\n",
    "\n",
    "dataset = wds.WebDataset(data, shardshuffle=False)\n",
    "\n",
    "dataloader = wds.WebLoader(dataset, num_workers=0, batch_size=None)\n",
    "\n",
    "print_samples_id(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28287a6a",
   "metadata": {},
   "source": [
    "### In-memory shuffling\n",
    "Each worker sequentially loads samples from one or more shards into batches (cannot load from 0 shards, will throw an error - hence make sure num workers<= num of shards).\n",
    "\n",
    "By using `.shuffle(buffer_size)`, a buffer can be created so that samples are loaded into it first, then randomly distributed into batches.\n",
    "\n",
    "Having a buffer size = to the number of samples into a shard allow to completely shuffle such shard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9bfecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_50094\n",
      "sample_50193\n",
      "sample_50184\n",
      "sample_50175\n",
      "sample_50177\n",
      "sample_50201\n",
      "sample_50102\n",
      "sample_50054\n",
      "sample_50166\n",
      "sample_50183\n"
     ]
    }
   ],
   "source": [
    "dataset = wds.WebDataset(data, shardshuffle=False)\n",
    "\n",
    "\n",
    "# Explanation of the initial argument: https://github.com/webdataset/webdataset/issues/62\n",
    "# Basically allows to await streaming data until the given amount of samples are shuffled.\n",
    "buffer_size = 200\n",
    "dataset = dataset.shuffle(buffer_size, initial=buffer_size)\n",
    "\n",
    "dataloader = wds.WebLoader(dataset, num_workers=0, batch_size=None)\n",
    "\n",
    "print_samples_id(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca609c",
   "metadata": {},
   "source": [
    "### Multiprocessing shuffling\n",
    "\n",
    "When the dataloader's number of workers parameter is set to 0, one shard is loaded into memory at a time. Consequently, the shards are accessed in the order in which the `.tar` files where loaded by `glob.glob()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9db507cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/cpg_methylation/hyenadna-tiny-1k/test_1.tar.gz',\n",
       " '../data/cpg_methylation/hyenadna-tiny-1k/test_2.tar.gz',\n",
       " '../data/cpg_methylation/hyenadna-tiny-1k/test_0.tar.gz']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33600db",
   "metadata": {},
   "source": [
    "However, increasing the number of workers to values greaten than 0, enables multiprocessing, and the number of shards accessed simultaneously is equal to the number of workers.\n",
    "\n",
    "This greatly increases randomisation, as samples of multiple shards are processed at once. If the number of shards = the number of workers, it allows to use samples from any __section__ of the dataset. \n",
    "\n",
    "However, each shard/section is accessed sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b82d759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_50000\n",
      "sample_100000\n",
      "sample_0\n",
      "sample_50001\n",
      "sample_100001\n",
      "sample_1\n",
      "sample_50002\n",
      "sample_100002\n",
      "sample_2\n",
      "sample_50003\n"
     ]
    }
   ],
   "source": [
    "dataset = wds.WebDataset(data, shardshuffle=False)\n",
    "\n",
    "dataloader = wds.WebLoader(dataset, num_workers=len(data), batch_size=None)\n",
    "\n",
    "print_samples_id(dataloader)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbadbc6e",
   "metadata": {},
   "source": [
    "Of course the `dataset.shuffle(buffer_size)` function can be used to allow the shuffling of the first N samples of each shard, where N=buffer_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48201efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_50130\n",
      "sample_100060\n",
      "sample_17\n",
      "sample_50152\n",
      "sample_100139\n",
      "sample_74\n",
      "sample_50102\n",
      "sample_100027\n",
      "sample_156\n",
      "sample_50028\n"
     ]
    }
   ],
   "source": [
    "dataset = wds.WebDataset(data, shardshuffle=False)\n",
    "\n",
    "buffer_size = 200\n",
    "dataset = dataset.shuffle(buffer_size, initial=buffer_size)\n",
    "\n",
    "dataloader = wds.WebLoader(dataset, num_workers=len(data), batch_size=None)\n",
    "\n",
    "print_samples_id(dataloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d80d8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
