defaults:
  - datadims: [label_dims, embedding_dims, downstream_downsample]
  #- hydra : multirun
  - supervised_encoder: [resnet-supervised, basset-supervised]
  #- override hydra/launcher: joblib # ability to launch jobs in parallel fashion
  - _self_
hydra:
  searchpath:
    - file://conf
  sweeper:
    params:
      embedder:
        hyenadna-tiny-1k,
        hyenadna-large-1m,
        resnetlm,
        nt_transformer_ms,
        nt_transformer_v2_500m,
        nt_transformer_human_ref,
        nt_transformer_1000g,
        dnabert2,
        awdlstm
        # gena-lm-bigbird-base-t2t,
        # gena-lm-bert-large-t2,
        # dnabert6,
        # onehot,
        # grover
        # hyenadna-small-32k,
        # hyenadna-medium-160k,
        # hyenadna-medium-450k
embedder: onehot
task: histone_modification
shuffle: false
work_dir: ./
output_dir: ${work_dir}/shuffle_experiment/default/ # Specify only the root output directory, script will generate the full path
model:
  _target_: bend.models.downstream.CNN
  encoder: null
  input_size: ${datadims.${embedder}}
  output_size: ${datadims.${task}}
  hidden_size: 64
  kernel_size: 3
  output_downsample_window: 512
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.003
  weight_decay: 0.01
data:
  _target_: bend.utils.data_downstream.get_data
  data_dir: ${work_dir}/data/ # Specify only the root data directory, script will generate the full path
  cross_validation: false
  batch_size: 256
  num_workers: 32
  padding_value: -100
  shuffle: 200
params:
  epochs: 100
  load_checkpoint: false
  mode: train
  gradient_accumulation_steps: 1
  criterion: bce
  class_weights: null
  metric: auroc
  activation: none
wandb:
  mode: disabled
